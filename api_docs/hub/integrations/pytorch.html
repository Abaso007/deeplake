<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>hub.integrations.pytorch API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hub.integrations.pytorch</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from hub.util.remove_cache import remove_memory_cache
from hub.util.join_chunks import join_chunks
from hub.core.meta.tensor_meta import read_tensor_meta
import os
import numpy as np
from itertools import repeat
from collections import defaultdict
from typing import Any, Callable, List, Optional, Set, Dict
from hub.core.meta.index_map import read_index_map
from hub.util.exceptions import ModuleNotInstalledException
from hub.util.shared_memory import (
    remove_shared_memory_from_resource_tracker,
    clear_shared_memory,
)
from pathos.pools import ProcessPool  # type: ignore
from hub.core.storage import MemoryProvider

try:
    from multiprocessing.shared_memory import SharedMemory  # type: ignore
except ModuleNotFoundError:
    pass

_hub_storage_provider = MemoryProvider()

# TODO make this use shared memory to make on the fly transforms faster. Currently using transform slows us down by 10x
def _apply_transform(transform: Callable, sample: Dict):
    &#34;&#34;&#34;Used to apply transforms to a single sample&#34;&#34;&#34;
    return transform(sample) if transform else sample


def _read_and_store_chunk(chunk_name: str, shared_memory_name: str, key: str):
    &#34;&#34;&#34;Reads a single chunk from the dataset&#39;s storage provider and stores it in the SharedMemory. Returns its size&#34;&#34;&#34;
    remove_shared_memory_from_resource_tracker()
    chunk_path = os.path.join(key, &#34;chunks&#34;, chunk_name)
    chunk_bytes = _hub_storage_provider[chunk_path]
    chunk_size = len(chunk_bytes)
    shm = SharedMemory(create=True, size=chunk_size, name=shared_memory_name)

    # needs to be done as some OS allocate extra space
    shm.buf[0:chunk_size] = chunk_bytes
    shm.close()
    return chunk_size


def dataset_to_pytorch(dataset, transform: Callable = None, workers: int = 1):
    return TorchDataset(dataset, transform, workers)


class TorchDataset:
    def __init__(self, dataset, transform: Callable = None, workers: int = 1):
        self.dataset = dataset
        self._set_globals()
        self.transform: Optional[Callable] = transform
        self.workers: int = workers
        self.map = ProcessPool(nodes=workers).map
        self.len = len(dataset)
        self.keys = list(self.dataset.tensors)

        # contains meta for each Tensor
        self.all_meta: Dict[str, Dict] = self._load_all_meta()

        # contains index_map for each Tensor
        self.all_index_maps: Dict[str, List] = self._load_all_index_maps()

        # stores index-value map for each Tensor where value is the actual array at the index
        # acts as in memory prefetch cache
        self.all_index_value_maps: Dict[str, Dict[int, Any]] = defaultdict(dict)

        # tracks last index that was prefetched in the prefetch cache for each Tensor
        self.last_index_map: Dict[str, int] = {}

        # in memory processed cache containing all samples generated after prefetching and transforming
        self.processed_samples: List[Dict] = []
        self.processed_range = slice(-1, -1)  # range of processed_samples

        # keeps track of names of all shared_memory that have data in them
        self.all_shared_memory_names: Dict[str, List[str]] = defaultdict(list)

        # keeps pointers to shared memory across tensors so they don&#39;t get closed between calls to getitem
        self.all_shared_memory: Dict = defaultdict(list)

        self.last_chunk_num_generated = -1

    def __len__(self):
        return self.len

    def __getitem__(self, index: int):
        for key in self.keys:
            # prefetch cache miss, fetch data
            if index not in self.all_index_value_maps[key]:
                self._prefetch_data(key, index)

        # processed cache miss, process more samples
        if index &gt; self.processed_range.stop:
            self._process_samples()
        if index == len(self) - 1:  # clean up at the end
            self._all_shared_memory_clean_up()
        return self.processed_samples[index - self.processed_range.start]

    def __iter__(self):
        for index in range(len(self)):
            yield self[index]

    # helper functions
    def _set_globals(self):
        &#34;&#34;&#34;Sets the global values for storage provider and a few plugins&#34;&#34;&#34;
        global torch
        try:
            import torch
        except ModuleNotFoundError:
            raise ModuleNotInstalledException(
                &#34;&#39;torch&#39; should be installed to convert the Dataset into pytorch format&#34;
            )

        # global to pass to processes, not possible to serialize and send
        global _hub_storage_provider

        # TODO boto3.client isn&#39;t safe for multiprocessing https://github.com/boto/boto3/pull/2848/files
        # could it be working here as we&#39;re only reading data?
        _hub_storage_provider = remove_memory_cache(self.dataset.storage)

    def _load_all_index_maps(self):
        &#34;&#34;&#34;Loads index maps for all Tensors into memory&#34;&#34;&#34;
        all_index_maps = {
            key: read_index_map(key, _hub_storage_provider) for key in self.keys
        }
        return all_index_maps

    def _load_all_meta(self):
        &#34;&#34;&#34;Loads meta for all Tensors into memory&#34;&#34;&#34;
        all_meta = {}
        # pytorch doesn&#39;t support certain dtypes, which are type casted to another dtype implicitly
        for key in self.keys:
            meta = read_tensor_meta(key, _hub_storage_provider)
            if meta[&#34;dtype&#34;] == &#34;uint16&#34;:
                meta[&#34;dtype&#34;] = &#34;int32&#34;
            elif meta[&#34;dtype&#34;] in [&#34;uint32&#34;, &#34;uint64&#34;]:
                meta[&#34;dtype&#34;] = &#34;int64&#34;
            all_meta[key] = meta
        return all_meta

    def _prefetch_data(self, key: str, index: int):
        &#34;&#34;&#34;Prefetches data for the given key, starting from the given index&#34;&#34;&#34;
        # clear data from previous prefetching, before fetching data
        del self.all_index_value_maps[key]
        old_shared_memory_names = self.all_shared_memory_names[key]
        clear_shared_memory(old_shared_memory_names)
        chunk_names = list(self._get_chunk_names(index, key))
        shared_memory_names = self._generate_shared_memory_names(chunk_names)
        clear_shared_memory(shared_memory_names)
        chunk_sizes: List[int] = self.map(
            _read_and_store_chunk, chunk_names, shared_memory_names, repeat(key)
        )
        self._get_data_from_chunks(
            index, key, chunk_names, shared_memory_names, chunk_sizes
        )
        self.all_shared_memory_names[key] = shared_memory_names

    def _generate_shared_memory_names(self, chunk_names: List[str]):
        &#34;&#34;&#34;Generates a name for every chunk_name as chunknames very large and fail on MacOS&#34;&#34;&#34;
        ls = []
        for _ in chunk_names:
            self.last_chunk_num_generated += 1
            ls.append(f&#34;al_{self.last_chunk_num_generated}&#34;)
        return ls

    def _get_chunk_names(self, index: int, key: str):
        &#34;&#34;&#34;Gets chunk names for elements starting from index to read in parallel&#34;&#34;&#34;
        chunk_names: Set[str] = set()
        index_map = self.all_index_maps[key]
        while len(chunk_names) &lt; self.workers and index &lt; len(self):
            chunks = index_map[index][&#34;chunk_names&#34;]
            chunk_names.update(chunks)
            index += 1
        return chunk_names

    def _np_from_chunk_list(self, index: int, key: str, chunks: List[bytes]):
        &#34;&#34;&#34;Takes a list of chunks and returns a numpy array from it&#34;&#34;&#34;
        index_entry = self.all_index_maps[key][index]

        start_byte = index_entry[&#34;start_byte&#34;]
        end_byte = index_entry[&#34;end_byte&#34;]
        dtype = self.all_meta[key][&#34;dtype&#34;]
        shape = index_entry[&#34;shape&#34;]

        combined_bytes = join_chunks(chunks, start_byte, end_byte)
        if isinstance(combined_bytes, memoryview):
            arr = np.frombuffer(combined_bytes, dtype=dtype).reshape(shape)
            combined_bytes.release()
        else:
            arr = np.frombuffer(combined_bytes, dtype=dtype).reshape(shape)
        return arr

    def _get_data_from_chunks(
        self,
        index: int,
        key: str,
        chunk_names: List[str],
        shared_memory_names: List[str],
        chunk_sizes: List[int],
    ):
        &#34;&#34;&#34;Extracts data from all the chunks in chunk_names and stores it index wise in a dictionary&#34;&#34;&#34;
        self.all_index_value_maps[key] = {}
        chunk_map = {}
        # loads value of chunks saved in SharedMemory into memory
        for chunk_name, shared_memory_name, chunk_size in zip(
            chunk_names, shared_memory_names, chunk_sizes
        ):
            self.all_shared_memory[key].append(SharedMemory(name=shared_memory_name))
            chunk_map[chunk_name] = self.all_shared_memory[key][-1].buf[:chunk_size]

        # saves np array for each index in memory
        for i in range(index, len(self)):
            chunks = []
            index_entry = self.all_index_maps[key][i]
            for chunk_name in index_entry[&#34;chunk_names&#34;]:
                if chunk_name not in chunk_map:
                    self.last_index_map[key] = i - 1
                    return
                chunks.append(chunk_map[chunk_name])
            self.all_index_value_maps[key][i] = self._np_from_chunk_list(i, key, chunks)

        self.last_index_map[key] = len(self) - 1

    def _process_samples(self):
        &#34;&#34;&#34;Processes the prefetched values from across tensors into dictionaries.
        These samples may be further processed if a transform is specified.
        &#34;&#34;&#34;
        first_index = self.processed_range.stop + 1
        # different no. of samples are fetched for each tensor, take the min and process
        last_index = min(self.last_index_map[key] for key in self.keys)
        samples = []
        for i in range(first_index, last_index + 1):
            sample = {key: self.all_index_value_maps[key][i] for key in self.keys}
            samples.append(sample)

        if self.transform:
            self.processed_samples = self.map(
                _apply_transform, repeat(self.transform), samples
            )
        else:
            self.processed_samples = samples
        self.processed_range = slice(first_index, last_index)

    def _all_shared_memory_clean_up(self):
        &#34;&#34;&#34;Cleans up possibly leaked memory at the end of iteration across Tensors&#34;&#34;&#34;
        for key in self.keys:
            shared_memory_names = self.all_shared_memory_names[key]
            clear_shared_memory(shared_memory_names)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hub.integrations.pytorch.dataset_to_pytorch"><code class="name flex">
<span>def <span class="ident">dataset_to_pytorch</span></span>(<span>dataset, transform: Callable = None, workers: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataset_to_pytorch(dataset, transform: Callable = None, workers: int = 1):
    return TorchDataset(dataset, transform, workers)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="hub.integrations.pytorch.TorchDataset"><code class="flex name class">
<span>class <span class="ident">TorchDataset</span></span>
<span>(</span><span>dataset, transform: Callable = None, workers: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TorchDataset:
    def __init__(self, dataset, transform: Callable = None, workers: int = 1):
        self.dataset = dataset
        self._set_globals()
        self.transform: Optional[Callable] = transform
        self.workers: int = workers
        self.map = ProcessPool(nodes=workers).map
        self.len = len(dataset)
        self.keys = list(self.dataset.tensors)

        # contains meta for each Tensor
        self.all_meta: Dict[str, Dict] = self._load_all_meta()

        # contains index_map for each Tensor
        self.all_index_maps: Dict[str, List] = self._load_all_index_maps()

        # stores index-value map for each Tensor where value is the actual array at the index
        # acts as in memory prefetch cache
        self.all_index_value_maps: Dict[str, Dict[int, Any]] = defaultdict(dict)

        # tracks last index that was prefetched in the prefetch cache for each Tensor
        self.last_index_map: Dict[str, int] = {}

        # in memory processed cache containing all samples generated after prefetching and transforming
        self.processed_samples: List[Dict] = []
        self.processed_range = slice(-1, -1)  # range of processed_samples

        # keeps track of names of all shared_memory that have data in them
        self.all_shared_memory_names: Dict[str, List[str]] = defaultdict(list)

        # keeps pointers to shared memory across tensors so they don&#39;t get closed between calls to getitem
        self.all_shared_memory: Dict = defaultdict(list)

        self.last_chunk_num_generated = -1

    def __len__(self):
        return self.len

    def __getitem__(self, index: int):
        for key in self.keys:
            # prefetch cache miss, fetch data
            if index not in self.all_index_value_maps[key]:
                self._prefetch_data(key, index)

        # processed cache miss, process more samples
        if index &gt; self.processed_range.stop:
            self._process_samples()
        if index == len(self) - 1:  # clean up at the end
            self._all_shared_memory_clean_up()
        return self.processed_samples[index - self.processed_range.start]

    def __iter__(self):
        for index in range(len(self)):
            yield self[index]

    # helper functions
    def _set_globals(self):
        &#34;&#34;&#34;Sets the global values for storage provider and a few plugins&#34;&#34;&#34;
        global torch
        try:
            import torch
        except ModuleNotFoundError:
            raise ModuleNotInstalledException(
                &#34;&#39;torch&#39; should be installed to convert the Dataset into pytorch format&#34;
            )

        # global to pass to processes, not possible to serialize and send
        global _hub_storage_provider

        # TODO boto3.client isn&#39;t safe for multiprocessing https://github.com/boto/boto3/pull/2848/files
        # could it be working here as we&#39;re only reading data?
        _hub_storage_provider = remove_memory_cache(self.dataset.storage)

    def _load_all_index_maps(self):
        &#34;&#34;&#34;Loads index maps for all Tensors into memory&#34;&#34;&#34;
        all_index_maps = {
            key: read_index_map(key, _hub_storage_provider) for key in self.keys
        }
        return all_index_maps

    def _load_all_meta(self):
        &#34;&#34;&#34;Loads meta for all Tensors into memory&#34;&#34;&#34;
        all_meta = {}
        # pytorch doesn&#39;t support certain dtypes, which are type casted to another dtype implicitly
        for key in self.keys:
            meta = read_tensor_meta(key, _hub_storage_provider)
            if meta[&#34;dtype&#34;] == &#34;uint16&#34;:
                meta[&#34;dtype&#34;] = &#34;int32&#34;
            elif meta[&#34;dtype&#34;] in [&#34;uint32&#34;, &#34;uint64&#34;]:
                meta[&#34;dtype&#34;] = &#34;int64&#34;
            all_meta[key] = meta
        return all_meta

    def _prefetch_data(self, key: str, index: int):
        &#34;&#34;&#34;Prefetches data for the given key, starting from the given index&#34;&#34;&#34;
        # clear data from previous prefetching, before fetching data
        del self.all_index_value_maps[key]
        old_shared_memory_names = self.all_shared_memory_names[key]
        clear_shared_memory(old_shared_memory_names)
        chunk_names = list(self._get_chunk_names(index, key))
        shared_memory_names = self._generate_shared_memory_names(chunk_names)
        clear_shared_memory(shared_memory_names)
        chunk_sizes: List[int] = self.map(
            _read_and_store_chunk, chunk_names, shared_memory_names, repeat(key)
        )
        self._get_data_from_chunks(
            index, key, chunk_names, shared_memory_names, chunk_sizes
        )
        self.all_shared_memory_names[key] = shared_memory_names

    def _generate_shared_memory_names(self, chunk_names: List[str]):
        &#34;&#34;&#34;Generates a name for every chunk_name as chunknames very large and fail on MacOS&#34;&#34;&#34;
        ls = []
        for _ in chunk_names:
            self.last_chunk_num_generated += 1
            ls.append(f&#34;al_{self.last_chunk_num_generated}&#34;)
        return ls

    def _get_chunk_names(self, index: int, key: str):
        &#34;&#34;&#34;Gets chunk names for elements starting from index to read in parallel&#34;&#34;&#34;
        chunk_names: Set[str] = set()
        index_map = self.all_index_maps[key]
        while len(chunk_names) &lt; self.workers and index &lt; len(self):
            chunks = index_map[index][&#34;chunk_names&#34;]
            chunk_names.update(chunks)
            index += 1
        return chunk_names

    def _np_from_chunk_list(self, index: int, key: str, chunks: List[bytes]):
        &#34;&#34;&#34;Takes a list of chunks and returns a numpy array from it&#34;&#34;&#34;
        index_entry = self.all_index_maps[key][index]

        start_byte = index_entry[&#34;start_byte&#34;]
        end_byte = index_entry[&#34;end_byte&#34;]
        dtype = self.all_meta[key][&#34;dtype&#34;]
        shape = index_entry[&#34;shape&#34;]

        combined_bytes = join_chunks(chunks, start_byte, end_byte)
        if isinstance(combined_bytes, memoryview):
            arr = np.frombuffer(combined_bytes, dtype=dtype).reshape(shape)
            combined_bytes.release()
        else:
            arr = np.frombuffer(combined_bytes, dtype=dtype).reshape(shape)
        return arr

    def _get_data_from_chunks(
        self,
        index: int,
        key: str,
        chunk_names: List[str],
        shared_memory_names: List[str],
        chunk_sizes: List[int],
    ):
        &#34;&#34;&#34;Extracts data from all the chunks in chunk_names and stores it index wise in a dictionary&#34;&#34;&#34;
        self.all_index_value_maps[key] = {}
        chunk_map = {}
        # loads value of chunks saved in SharedMemory into memory
        for chunk_name, shared_memory_name, chunk_size in zip(
            chunk_names, shared_memory_names, chunk_sizes
        ):
            self.all_shared_memory[key].append(SharedMemory(name=shared_memory_name))
            chunk_map[chunk_name] = self.all_shared_memory[key][-1].buf[:chunk_size]

        # saves np array for each index in memory
        for i in range(index, len(self)):
            chunks = []
            index_entry = self.all_index_maps[key][i]
            for chunk_name in index_entry[&#34;chunk_names&#34;]:
                if chunk_name not in chunk_map:
                    self.last_index_map[key] = i - 1
                    return
                chunks.append(chunk_map[chunk_name])
            self.all_index_value_maps[key][i] = self._np_from_chunk_list(i, key, chunks)

        self.last_index_map[key] = len(self) - 1

    def _process_samples(self):
        &#34;&#34;&#34;Processes the prefetched values from across tensors into dictionaries.
        These samples may be further processed if a transform is specified.
        &#34;&#34;&#34;
        first_index = self.processed_range.stop + 1
        # different no. of samples are fetched for each tensor, take the min and process
        last_index = min(self.last_index_map[key] for key in self.keys)
        samples = []
        for i in range(first_index, last_index + 1):
            sample = {key: self.all_index_value_maps[key][i] for key in self.keys}
            samples.append(sample)

        if self.transform:
            self.processed_samples = self.map(
                _apply_transform, repeat(self.transform), samples
            )
        else:
            self.processed_samples = samples
        self.processed_range = slice(first_index, last_index)

    def _all_shared_memory_clean_up(self):
        &#34;&#34;&#34;Cleans up possibly leaked memory at the end of iteration across Tensors&#34;&#34;&#34;
        for key in self.keys:
            shared_memory_names = self.all_shared_memory_names[key]
            clear_shared_memory(shared_memory_names)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hub.integrations" href="index.html">hub.integrations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hub.integrations.pytorch.dataset_to_pytorch" href="#hub.integrations.pytorch.dataset_to_pytorch">dataset_to_pytorch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="hub.integrations.pytorch.TorchDataset" href="#hub.integrations.pytorch.TorchDataset">TorchDataset</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>